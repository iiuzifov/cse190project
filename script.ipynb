{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1764,
   "id": "88fe97bc-4882-4b3d-b77a-61ece366e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from music21 import converter, instrument, stream, note, chord\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation, Bidirectional, Flatten, Embedding, Lambda, Input, concatenate, Reshape, Permute, RepeatVector, Multiply\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from fractions import Fraction\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1765,
   "id": "4c9891ad-fca1-4566-9755-1a8116eff2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_midi_to_notes():\n",
    "    notes = []\n",
    "    for file in glob.glob(\"classical/*.mid\"):\n",
    "        print(\"Parsing %s\" % file)\n",
    "        midi = converter.parse(file)\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            length = str(element.quarterLength)\n",
    "            if \"/\" in str(element.quarterLength):\n",
    "                length = str('%.1f' % float(element.quarterLength))\n",
    "                \n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch) + \" \" +  length)\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder) + \" \" + length)\n",
    "            elif isinstance(element, note.Rest):\n",
    "                notes.append(str(element.name)  + \" \" + length)\n",
    "  \n",
    "    #pickle.dump(notes, open('notes.p', 'wb'))\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1766,
   "id": "32e5539b-2bc8-42c5-9b0b-8ab2fe06ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network1(network_input, n_vocab): #attention network\n",
    "  \n",
    "    notes_in = Input(shape = (network_input.shape[1]))\n",
    "    \n",
    "  \n",
    "    x1 = Embedding(n_vocab, 5, input_length = network_input.shape[1])(notes_in)\n",
    "\n",
    "    x = LSTM(512, return_sequences = True)(x1)\n",
    "    x = LSTM(512, return_sequences = True)(x)\n",
    "    \n",
    "    e = Dense(1, activation='tanh')(x)\n",
    "    e = Reshape([-1])(e)\n",
    "   \n",
    "\n",
    "    alpha = Activation('softmax')(e)\n",
    "\n",
    "    c = Permute([2,1])(RepeatVector(512)(alpha))\n",
    "    c = Multiply()([x,c])\n",
    " \n",
    "    c = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(512,))(c)\n",
    "    \n",
    "    notes_out = Dense(n_vocab, activation = 'softmax')(c)\n",
    "\n",
    "    model = Model(notes_in, notes_out)\n",
    "\n",
    "    att_model = Model(notes_in, alpha)\n",
    "\n",
    "    #opti = RMSprop(lr = 0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1767,
   "id": "c9df21b9-51fd-4aa8-bffc-1d3e43de1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab): #sequential network\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        recurrent_dropout=0.3,\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(LSTM(512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]), \n",
    "        recurrent_dropout=0.3,\n",
    "        return_sequences=False\n",
    "    ))\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Lambda(lambda x: x / 0.6))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1768,
   "id": "321c7f2c-f259-4645-bf0a-12bb83f5d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
    "    #notes = convert_midi_to_notes()\n",
    "    n_vocab = len(set(notes))\n",
    "    \n",
    "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "    \n",
    "    model = create_network(network_input, n_vocab)\n",
    " \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        #\"weights2-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\",\n",
    "        \"weights.hdf5\",\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "# Your line of code here\n",
    "    model.fit(network_input, network_output, epochs = 5,callbacks=[callbacks_list])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1769,
   "id": "ff940385-a96b-45d5-930a-de6b88ac4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 4\n",
    "\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1770,
   "id": "dc4dd7b8-cdf2-44c6-b400-aa3aa71194a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_prediction(notes, pitchnames, n_vocab):\n",
    "\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    sequence_length = 4\n",
    "    network_input = []\n",
    "    output = []\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    normalized_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "\n",
    "    return (network_input, normalized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1771,
   "id": "4fbb1be0-8d49-4afb-b859-0649dbd583ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # Starts the melody by picking a random sequence from the input as a starting point\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "\n",
    "    for note_index in range(200):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "        prediction = model.predict(prediction_input)       \n",
    "        index = np.random.choice(range(len(prediction[0])), p=prediction[0])\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1772,
   "id": "eb8f4833-9486-4d1e-b765-cf9e29570df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_music_file(prediction_output):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern_and_duration in prediction_output:\n",
    "        duration = pattern_and_duration.split()[1]\n",
    "        pattern = pattern_and_duration.split()[0]\n",
    "\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            new_chord.quarterLength = float(duration)\n",
    "            output_notes.append(new_chord)\n",
    "        elif('rest' in pattern):\n",
    "            new_rest = note.Rest(pattern)\n",
    "            new_rest.offset = offset\n",
    "            new_rest.storedInstrument = instrument.Piano()\n",
    "            new_rest.quarterLength = float(duration)\n",
    "            output_notes.append(new_rest)\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            new_note.quarterLength = float(duration)\n",
    "            output_notes.append(new_note)\n",
    "        offset += float(duration)\n",
    "\n",
    "        \n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='test_output.mid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1773,
   "id": "ffbbb7f9-4c68-4e92-99b4-d74f1558b412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing classical/appass_2_format0.mid\n",
      "Parsing classical/beethoven_hammerklavier_1_format0.mid\n",
      "Parsing classical/beethoven_hammerklavier_2_format0.mid\n",
      "Parsing classical/appass_1_format0.mid\n",
      "Parsing classical/appass_3_format0.mid\n",
      "Parsing classical/beethoven_hammerklavier_3_format0.mid\n"
     ]
    }
   ],
   "source": [
    "# MAIN BEGINS\n",
    "# notes = convert_midi_to_notes()\n",
    "notes = convert_midi_to_notes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1774,
   "id": "8f7223f3-d368-4ac1-bd2e-91b73937b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10208 samples\n",
      "Epoch 1/5\n",
      "10208/10208 [==============================] - 72s 7ms/sample - loss: 6.5822\n",
      "Epoch 2/5\n",
      "10208/10208 [==============================] - 36s 4ms/sample - loss: 6.0697\n",
      "Epoch 3/5\n",
      "10208/10208 [==============================] - 38s 4ms/sample - loss: 5.8607\n",
      "Epoch 4/5\n",
      "10208/10208 [==============================] - 39s 4ms/sample - loss: 5.7601\n",
      "Epoch 5/5\n",
      "10208/10208 [==============================] - 39s 4ms/sample - loss: 5.6427\n"
     ]
    }
   ],
   "source": [
    "train_model()\n",
    "#notes = pickle.load(open('notes.p', 'rb'))\n",
    "\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "n_vocab = len(set(notes))\n",
    "\n",
    "network_input, normalized_input = prepare_sequences_prediction(notes, pitchnames, n_vocab)\n",
    "model = create_network(normalized_input, n_vocab)\n",
    "\n",
    "model.load_weights('weights.hdf5')\n",
    "\n",
    "prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1775,
   "id": "4bf2a99a-2252-48bf-ab28-330aea325b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_music_file(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1776,
   "id": "95032348-4dec-4863-aa09-e7825ff80120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimental code for multiple instruments\n",
    "\n",
    "#     midi = converter.parse('test_output.mid')\n",
    "#     for el in midi.recurse():\n",
    "#         if 'Instrument' in el.classes: # or 'Piano'\n",
    "#             el.activeSite.replace(el, instrument.SteelDrum())\n",
    "#     midi.parts[0].insert(0, instrument.SteelDrum())\n",
    "#     midi.parts[1].insert(0, instrument.Guitar())\n",
    "#     for p in midi.parts:\n",
    "#         p.insert(0, instrument.SteelDrum())\n",
    "    \n",
    "#     midi.write('midi', fp='test_output2.mid')\n",
    "    \n",
    "# def generate_second_instrument():\n",
    "#     notes = pickle.load(open('notes.p', 'rb'))\n",
    "#     pitchnames = sorted(set(item for item in notes))\n",
    "#     n_vocab = len(set(notes))\n",
    "#     network_input, normalized_input = prepare_sequences_prediction(notes, pitchnames, n_vocab)\n",
    "#     model = create_network(normalized_input, n_vocab)\n",
    "#     model.load_weights('double_weights.hdf5')\n",
    "#     prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "#     create_midi(prediction_output)\n",
    "# def get_notes_second_instrument():\n",
    "#     notes = []\n",
    "\n",
    "#     for file in glob.glob(\"rock_test/*.mid\"):\n",
    "#         midi = converter.parse(file)\n",
    "\n",
    "#         print(\"Parsing %s\" % file)\n",
    "\n",
    "#         elements_to_parse = None\n",
    "\n",
    "#         try: # file has instrument parts\n",
    "#             s2 = instrument.partitionByInstrument(midi) \n",
    "#             guitar = s2.parts[0].recurse() \n",
    "#             drums = = s2.parts[1].recurse()\n",
    "#         except: # file has notes in a flat structure\n",
    "#             elements_to_parse = midi.flat.notes\n",
    "\n",
    "#         for i in (0, len(drums): drums[i]\n",
    "#             length = str(drums[i].quarterLength)\n",
    "#             if \"/\" in str(drums[i].quarterLength):\n",
    "#                 length = str('%.1f' % float(drums[i].quarterLength))\n",
    "                \n",
    "#             if isinstance(drums[i], note.Note):\n",
    "#                 notes.append(str(drums[i].pitch) + \" \" +  length + str(guitar[i].pitch))\n",
    "#             elif isinstance(drums[i], chord.Chord):\n",
    "#                 notes.append('.'.join(str(n) for n in drums[i].normalOrder) + \" \" + length \n",
    "#                     + '.'.join(str(n) for n in guitar[i].normalOrder))\n",
    "#             elif isinstance(drums[i], note.Rest):\n",
    "#                 notes.append(str(drums[i].name)  + \" \" + length + str(guitar[i].name))\n",
    "#     pickle.dump(notes, open('notes.p', 'wb'))\n",
    "#     return notes\n",
    "# def create_midi(prediction_output):\n",
    "#     properly unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1777,
   "id": "d7276fdb-1293-4990-be12-019f0a5d1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other attention layer attempts\n",
    "def create_network3(network_input, n_vocab):\n",
    "    ipt = Input(shape=(network_input.shape[1], network_input.shape[2])) \n",
    "    x = LSTM(512, activation='tanh', return_sequences=True)(ipt) \n",
    "    x = SeqSelfAttention(return_attention=True)(x) \n",
    "    x = concatenate(x) \n",
    "    x = Flatten()(x) \n",
    "    print(x.shape)\n",
    "    out = Dense(1, activation='sigmoid')(x) \n",
    "    model = Model(ipt,out) \n",
    "    model.compile(optimizer = 'adorn', loss = 'categorical_crossentropy') \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1778,
   "id": "f1f27271-9daa-4ff4-bf75-5d6bd326389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network2(network_input, n_vocab):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]), #n_time_steps, n_features?\n",
    "        return_sequences=True)))\n",
    "    #model.add(Bidirectional(LSTM(10)))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1779,
   "id": "64a2ed4e-6fb3-4c7e-ac84-63a261b7971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network4(network_input, n_vocab):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=network_input.shape[0],\n",
    "                                     output_dim=network_input.shape[1],\n",
    "                                     mask_zero=True))\n",
    "    model.add(Bidirectional(LSTM(units=512,\n",
    "                                                           return_sequences=True)))\n",
    "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "    model.add(Dense(units=256))\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
